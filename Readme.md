# ğŸ§  MedBookGPT

MedBookGPT is a powerful **multimodal Retrieval-Augmented Generation (RAG)** web application that allows users to query medical books and receive intelligent, context-aware responses. It supports both **textual content** and **visual content** (figures, tables, charts) extracted from medical PDFs, making it an ideal assistant for students and professionals in the medical field.

---

## ğŸš€ Features

- ğŸ” **Text-Based Querying**: Search across large volumes of medical text using semantic similarity.
- ğŸ–¼ï¸ **Image-Aware Retrieval**: Understands and retrieves image descriptions (figures, tables, etc.) embedded within the book PDFs.
- ğŸ“š **Book-Level Metadata**: Each result includes contextual metadata such as the book title and page number.
- ğŸŒ **Vector DB Integration**: Uses Pinecone for fast and scalable vector search.
- âš™ï¸ **Multimodal Embedding Pipeline**: Combines text chunking with image description embeddings to enhance retrieval performance.
- ğŸ¤– **LLM-Powered Answering**: Powered by Gemini and the `all-mpnet-base-v2` embedding model.

---

## ğŸ§© Project Architecture

![Project Architecture](https://github.com/user-attachments/assets/57c7ac51-de9d-4b76-9be1-41eaa6eb935f)

> The above diagram illustrates the full end-to-end pipeline from PDF ingestion to multimodal retrieval and LLM-based answer generation.

---

## ğŸ“¼ Demo Video

*(https://drive.google.com/file/d/1COAF242QDSlv2GhBQtb3K62b1KZvU61a/view?usp=drive_link)*

---

## ğŸ› ï¸ Tech Stack

| Layer           | Technology                      |
|----------------|----------------------------------|
| Embedding Model| `all-mpnet-base-v2`              |
| Vector Store   | Pinecone                         |
| LLM            | Gemini                           |
| PDF Parsing    | `pymupdf4llm`, `text-splitter`   |
| Image Hosting  | Cloudinary                       |
| Backend        | Python, Flask/FastAPI (or specify)|
| Frontend       | (Optional: React, Streamlit, etc.)|

---


---

## ğŸ“¦ How It Works

1. **Text Extraction**: Each medical PDF is split into manageable chunks using a Recursive Character Text Splitter.
2. **Image Extraction**: Figures, tables, and visual elements are extracted using `pymupdf4llm`.
3. **Cloud Upload**: Extracted images are uploaded to Cloudinary and stored with their URLs.
4. **Embedding**:
   - Text chunks and image descriptions are embedded using `all-mpnet-base-v2`.
   - Metadata (e.g., image URLs, book title) is added during vector insertion.
5. **Vector Storage**: All embeddings are stored in Pinecone for fast retrieval.
6. **Query Handling**:
   - User submits a query â†’ embedded â†’ top-K similar results fetched from Pinecone.
   - Gemini LLM uses the retrieved context to generate a detailed answer.

---

## ğŸ“Œ Use Cases

- ğŸ§‘â€âš•ï¸ Medical Students preparing for exams
- ğŸ“– Professionals looking for quick reference
- ğŸ‘©â€ğŸ« Teachers creating content from reliable academic sources

---

## ğŸ§ª Future Improvements

- ğŸ”Š Add voice input and TTS for query and response
- ğŸ“± Launch a mobile-friendly interface
- ğŸ§¬ Fine-tune model with medical QA datasets for improved accuracy
- ğŸ§  Expand to include more disciplines (Law, Engineering, etc.)

---

## ğŸ“¥ Installation & Running Locally

```bash
git clone https://github.com/yourusername/medbookgpt.git
cd medbookgpt
pip install -r requirements.txt
python app/main.py


